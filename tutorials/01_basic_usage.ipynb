{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalverse Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Evalverse library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evalverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Check the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalverse.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get a evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evalverse.Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Run an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-29 22:29:49][INFO][evalverse - evaluator.py:113] >> The value of argument \"h6_en\" has been changed to \"True\".\n",
      "[2024-03-29 22:29:49][INFO][evalverse - evaluator.py:131] >> Args {'ckpt_path': 'upstage/SOLAR-10.7B-Instruct-v1.0', 'output_path': '/data/private/new_lib/evalverse/tutorials/results', 'model_name': 'SOLAR-10.7B-Instruct-v1.0', 'use_fast_tokenizer': False, 'devices': '0', 'use_flash_attention_2': False, 'h6_en': True, 'batch_size': 16, 'use_vllm': False, 'gpu_memory_utilization': 0.8, 'model_parallel': 1, 'data_parallel': 1, 'load_in_8bit': False, 'load_in_4bit': False, 'mt_bench': False, 'baselines': None, 'judge_model': 'gpt-4', 'num_gpus_total': 1, 'num_gpus_per_model': 1, 'parallel_api': 1, 'ifeval': False, 'gpu_per_inst_eval': 1, 'eq_bench': False, 'eq_bench_prompt_type': 'ChatML', 'eq_bench_lora_path': None, 'eq_bench_quantization': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_eval --model hf --model_args pretrained=upstage/SOLAR-10.7B-Instruct-v1.0,trust_remote_code=True,dtype=float16,use_fast_tokenizer=False,use_flash_attention_2=False --tasks arc_challenge --batch_size 16 --num_fewshot 25 --output_path /data/private/new_lib/evalverse/tutorials/results/SOLAR-10.7B-Instruct-v1.0/h6_en/arc_challenge_25.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29:22:29:58,786 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-03-29:22:30:03,328 INFO     [__main__.py:335] Selected Tasks: ['arc_challenge']\n",
      "2024-03-29:22:30:03,328 INFO     [__main__.py:336] Loading selected tasks...\n",
      "2024-03-29:22:30:03,333 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n"
     ]
    }
   ],
   "source": [
    "model = \"upstage/SOLAR-10.7B-Instruct-v1.0\"\n",
    "benchmark = \"h6_en\"\n",
    "\n",
    "evaluator.run(model=model, benchmark=benchmark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
